{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"lab_semseg_en.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"L2xatZg6iSEj","colab_type":"text"},"source":["![PyTorch Logo](images/pytorch1.png)"]},{"cell_type":"code","metadata":{"id":"STgQbdaFiSEr","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm, tqdm_notebook"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J0dL28cJiSEv","colab_type":"text"},"source":["We will use not only basic functionlaity of `pytorch` but also **`torchvision`** computer vision library."]},{"cell_type":"code","metadata":{"id":"3jiWFrd9iSEw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"0849849a-20ec-4857-f691-5ab08cc7549b","executionInfo":{"status":"ok","timestamp":1570978517919,"user_tz":-180,"elapsed":7407,"user":{"displayName":"тимур аминов","photoUrl":"","userId":"10861342555234053496"}}},"source":["!pip freeze 2>/dev/null | grep torch"],"execution_count":3,"outputs":[{"output_type":"stream","text":["torch==1.2.0\n","torchsummary==1.5.1\n","torchtext==0.3.1\n","torchvision==0.4.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Y2BFB0hRiSE7","colab_type":"text"},"source":["# Pytorch as a constructor"]},{"cell_type":"markdown","metadata":{"id":"gf3C8Z-yiSE8","colab_type":"text"},"source":["## Working with datasets\n","\n","For data loading pytorch defines **`Dataset`** entity.\n","\n","This abstract class is defined in `torch.utils.data.dataset`:\n","\n","```python\n","class Dataset(object):\n","    \"\"\"An abstract class representing a Dataset.\n","\n","    All other datasets should subclass it. All subclasses should override\n","    ``__len__``, that provides the size of the dataset, and ``__getitem__``,\n","    supporting integer indexing in range from 0 to len(self) exclusive.\n","    \"\"\"\n","\n","    def __getitem__(self, index):\n","        raise NotImplementedError\n","\n","    def __len__(self):\n","        raise NotImplementedError\n","\n","    def __add__(self, other):\n","        return ConcatDataset([self, other])\n","```\n","One should inherit `Dataset` and implement `__getitem__` and `__len__` nethods to create a new data source.\n","\n","An example of such ancestor — `torchvision.datasets.ImageFolder`, which allows us to use imagenet-like dataset based on a directory with `./train/{class}` and `./val/{class}` sub-directories structure:\n","\n","```python\n","imagenet = torchvision.datasets.ImageFolder('path/to/imagenet_root/')\n","```\n","\n","Custom example — a dataset loading images with classes defined in some text file:"]},{"cell_type":"code","metadata":{"id":"o0SqwuKsiSE8","colab_type":"code","colab":{}},"source":["from torch.utils.data import Dataset\n","# default_loader - default image loading function, uses accimage or PIL\n","from torchvision.datasets.folder import default_loader\n","\n","class TxtList(Dataset):\n","    def __init__(self, path, transform=None, loader=default_loader):\n","        with open(path) as fin:\n","            self.imgs = [s.strip().split() for s in fin.readlines()]\n","\n","        print(f'=> Found {len(self.imgs)} entries in {path}')\n","\n","        self.classes = sorted(set([_[1] for _ in self.imgs]))\n","        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n","        self.transform = transform\n","        self.loader = loader\n","\n","    def __getitem__(self, index):\n","        path, target = self.imgs[index]\n","        target = self.class_to_idx[target]\n","        img = self.loader(path)\n","        if self.transform is not None:\n","            img = self.transform(img)\n","\n","        return img, target\n","\n","    def __len__(self):\n","        return len(self.imgs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RXb5bA0ZiSFA","colab_type":"code","colab":{}},"source":["!echo '/tmp/1.jpg\\tcat' > /tmp/dataset.tsv\n","!echo '/tmp/2.jpg\\tcat' >> /tmp/dataset.tsv\n","!echo '/tmp/3.jpg\\tdog' >> /tmp/dataset.tsv\n","!echo '/tmp/4.jpg\\tcat' >> /tmp/dataset.tsv"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-_SXEx7AiSFJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":323},"outputId":"378bd8c9-98c1-4804-e352-cb8dcf2d9924","executionInfo":{"status":"error","timestamp":1570978528654,"user_tz":-180,"elapsed":16042,"user":{"displayName":"тимур аминов","photoUrl":"","userId":"10861342555234053496"}}},"source":["catdog = TxtList('/tmp/dataset.tsv')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["=> Found 4 entries in /tmp/dataset.tsv\n"],"name":"stdout"},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-452e3f26c896>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcatdog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTxtList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/tmp/dataset.tsv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-4-eb80aa5ad2c9>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, transform, loader)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'=> Found {len(self.imgs)} entries in {path}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-eb80aa5ad2c9>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'=> Found {len(self.imgs)} entries in {path}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]},{"cell_type":"code","metadata":{"id":"LfYS_sv4iSFN","colab_type":"code","colab":{}},"source":["catdog.classes"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z2TwBvhEiSFP","colab_type":"code","colab":{}},"source":["catdog.imgs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SvGwGzP9iSFT","colab_type":"code","colab":{}},"source":["len(catdog)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vRu4dRy-iSFW","colab_type":"code","colab":{}},"source":["# FileNotFoundError\n","catdog[0]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qt9sR0qXiSFe","colab_type":"text"},"source":["`torchvision` has another useful classes for using standard datasets: \n","http://pytorch.org/docs/master/torchvision/datasets.html.\n","\n","Some of them can be preloaded with built-in functionality, for example **MNIST**:"]},{"cell_type":"code","metadata":{"id":"Pt-eXtdJiSFf","colab_type":"code","colab":{}},"source":["![ -d '/tmp/mnist/' ] && rm -r '/tmp/mnist/'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wLMeOAdbiSFi","colab_type":"code","colab":{}},"source":["from torchvision.datasets import MNIST\n","\n","%time mnist = MNIST('/tmp/mnist/', train=True, download=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PzZI1rUkiSFn","colab_type":"code","colab":{}},"source":["len(mnist)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dTfSofWxiSFw","colab_type":"code","colab":{}},"source":["type(mnist)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8_HBfYR4iSFz","colab_type":"code","colab":{}},"source":["image, target = mnist[0]\n","print(target)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"d4bqwMlGiSF2","colab_type":"code","colab":{}},"source":["plt.imshow(np.array(image), 'gray')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r0CCzTf_iSF-","colab_type":"text"},"source":["## <font color='#cc6666'>Hometask!</font>"]},{"cell_type":"markdown","metadata":{"id":"Pg_wjs3SiSF_","colab_type":"text"},"source":["Implement **`UrlList`** dataset which costructor takes list of urls as a parameter."]},{"cell_type":"code","metadata":{"id":"zXXUHQqliSGA","colab_type":"code","colab":{}},"source":["import requests\n","link = \"http://2.bp.blogspot.com/-G_BQbLpOKtM/VL0FG0zSmkI/AAAAAAAAw-g/g3JZMK0yGCA/w620/1%2B%D0%BA%D1%80%D0%B0%D1%81%D0%B8%D0%B2%D1%8B%D0%B5%2B%D1%84%D0%BE%D1%82%D0%BE.jpg\"\n","p = requests.get(link)\n","out = open(\"./data/1.jpg\" , \"wb\")\n","out.write(p.content)\n","out.close()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7wg3VodKiSGD","colab_type":"code","colab":{}},"source":["from typing import List\n","import requests\n","\n","\n","\n","class UrlList(Dataset):\n","    def __init__(self, urls: List[str]):\n","        for link in urls:\n","            p = requests.get(link)\n","            out = open(, \"wb\")\n","            out.write(p.content)\n","            out.close()\n","\n","    \n","    \n","    def __getitem__(self, index):\n","        path, target = self.imgs[index]\n","        target = self.class_to_idx[target]\n","        img = self.loader(path)\n","        if self.transform is not None:\n","            img = self.transform(img)\n","\n","        return img, target\n","\n","    def __len__(self):\n","        return len(self.imgs)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"16XrsgNliSGH","colab_type":"text"},"source":["Demonstrate how it works with some examples:"]},{"cell_type":"code","metadata":{"id":"Ckak8lI4iSGI","colab_type":"code","colab":{}},"source":["pass"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9DIV2H2ziSGQ","colab_type":"text"},"source":["## Data transformation\n","\n","In the example shown before and in built-in `ImageFolder` `__init__` methods has `transform` parameter (and `target_transform`).\n","\n","They are used to transform images/targets loaded into predefined range and form.\n","\n","There is `transforms` sub-module in `torchvision` library which has some examples of such transforms:"]},{"cell_type":"code","metadata":{"id":"DItRGxUpiSGS","colab_type":"code","colab":{}},"source":["from torchvision import transforms"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4B38hxXOiSGd","colab_type":"text"},"source":["For example `transforms.ToTensor()` transforms uint8 `PIL` [0, 256)-domained images into [0, 1)-domained tensors."]},{"cell_type":"code","metadata":{"id":"d3uBNVo5iSGg","colab_type":"code","colab":{}},"source":["to_tensor = transforms.ToTensor()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TE0Q6U8CiSGk","colab_type":"code","colab":{},"outputId":"57b08a26-3663-4f64-a430-73aad4b08020"},"source":["pil_image = mnist[0][0]\n","th_image = to_tensor(pil_image)\n","th_image.shape, th_image.min(), th_image.max()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([1, 28, 28]), tensor(0.), tensor(1.))"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"HwABajX3iSGm","colab_type":"text"},"source":["One can define `normalize` to implement a standard ImageNet preparation step:"]},{"cell_type":"code","metadata":{"id":"JTWE0On1iSGn","colab_type":"code","colab":{}},"source":["normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NwNtv-hYiSGw","colab_type":"text"},"source":["`transforms.Compose` is used to sequence several compositions as a whole:"]},{"cell_type":"code","metadata":{"id":"qljdIoKniSGy","colab_type":"code","colab":{}},"source":["crop_and_tensorize = transforms.Compose([\n","    transforms.CenterCrop(16),\n","    transforms.ToTensor(),\n","])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aYM3M0FsiSG1","colab_type":"code","colab":{},"outputId":"a17747ac-9e2d-4ce3-eb4c-b479d593af96"},"source":["plt.imshow(crop_and_tensorize(pil_image)[0].numpy(), 'gray')\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"5vp7ZVOEiSG3","colab_type":"text"},"source":["To define a custom transformer we should only implement `__call__` method its implementation:\n","\n","```python\n","class HorizontalFlip(object):\n","    def __init__(self, mode=0):\n","        self.method = mode\n","\n","    def __call__(self, img):\n","        \"\"\"\n","        Args:\n","            img (PIL.Image): Image to be flipped.\n","\n","        Returns:\n","            PIL.Image: Randomly flipped image.\n","        \"\"\"\n","        if self.method:\n","            return img.transpose(Image.FLIP_LEFT_RIGHT)\n","        return img\n","\n","```\n","\n","It is adviced to take a glance at http://pytorch.org/docs/master/torchvision/transforms.html for standard transformations overview."]},{"cell_type":"markdown","metadata":{"id":"0niPWRKCiSG4","colab_type":"text"},"source":["## Optional reading\n","\n","Another example of a good set of pre-defined transformations is [`albumentations`](https://github.com/albu/albumentations) library.\n","\n","It supports not only image transformations but it can simultaniously transform its corresponding masks and bboxes.\n","\n","One can also use [`imgaug`](https://github.com/aleju/imgaug) augmentations library but needs to implement `imguag output` -> `tensor` transformations on their own."]},{"cell_type":"markdown","metadata":{"id":"XItRko7_iSG5","colab_type":"text"},"source":["## <font color='#cc6666'>Hometask!</font>\n","\n","Implement a transformer that applies random transformation from $D_4$ transformations group.\n","\n","These transformations are very usefull for lossless augmentations in satellite images analysis."]},{"cell_type":"code","metadata":{"id":"kAbTylyqiSG_","colab_type":"code","colab":{}},"source":["from PIL import Image\n","class RandomD4(object):\n","    def __call__(self, img):\n","        pass"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nHaW2VHOiSHC","colab_type":"text"},"source":["Demonstrate how it works on some MNIST images:"]},{"cell_type":"code","metadata":{"id":"aIEucCGGiSHD","colab_type":"code","colab":{}},"source":["pass"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xsIQ01a5iSHG","colab_type":"text"},"source":["## Data loaders\n","\n","The main reason to implement `Dataset` class is a magic power of `torchvision` loaders:"]},{"cell_type":"code","metadata":{"id":"jI3IwWiWiSHH","colab_type":"code","colab":{}},"source":["from torch.utils.data import DataLoader"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aJDNcg2fiSHJ","colab_type":"text"},"source":["The loaders are built on top of some dataset and allow batch iterating over it.\n","\n","Those batches in their turn are created with applying the transformations defined in background processes.\n","\n","Let's look at MNIST dataset with a simple transformer applied as loaders use tensors and not `PIL.Image`s."]},{"cell_type":"code","metadata":{"id":"wPxAIytwiSHL","colab_type":"code","colab":{}},"source":["transformed_mnist = MNIST('/tmp/mnist/', train=True, transform=transforms.ToTensor())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"x_4qEZfmiSHQ","colab_type":"code","colab":{}},"source":["mnist_loader = DataLoader(transformed_mnist, batch_size=8, shuffle=True, num_workers=4)  # shuffle note here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z9OJzKJdiSHT","colab_type":"text"},"source":["DataLoader standard parameter values are shown above.\n","\n","One can also add **`pin_memory=True`** to page-lock the memory thus making faster cpu-to-cuda transfer with a non-blocking option.\n","\n","**`drop_last=True`** is for avoiding batches size skew in the training process.\n","\n","Let us demonstrate how it works:"]},{"cell_type":"code","metadata":{"id":"K-uGUfyfiSHU","colab_type":"code","colab":{},"outputId":"da504c4a-3832-4c62-9eb6-954592d7be6d"},"source":["for images, targets in tqdm(mnist_loader):\n","    pass"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████████████| 7500/7500 [00:04<00:00, 1598.41it/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"8kNfzajDiSHW","colab_type":"text"},"source":["There are image batches as tensors on the output:"]},{"cell_type":"code","metadata":{"id":"0xw-pnBDiSHX","colab_type":"code","colab":{},"outputId":"85278b96-48a2-432d-f6a9-b61ed6ff15b6"},"source":["images.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([8, 1, 28, 28])"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"code","metadata":{"id":"Mtre9O0viSHZ","colab_type":"code","colab":{},"outputId":"6139e7bf-65a0-4175-a402-09e3c56a282d"},"source":["targets.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([8])"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"markdown","metadata":{"id":"986CtQzniSHl","colab_type":"text"},"source":["The loader instance pre-calculate the total number of batches (what makes `tqdm` happy):"]},{"cell_type":"code","metadata":{"id":"Gkvtb8ujiSHl","colab_type":"code","colab":{},"outputId":"423acd55-d614-49c3-ec03-dd9cb1c9f5fb"},"source":["len(mnist_loader)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7500"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"markdown","metadata":{"id":"EU7w86d5iSHn","colab_type":"text"},"source":["Some data example (will be random on each run):"]},{"cell_type":"code","metadata":{"id":"pFcJAVE5iSHp","colab_type":"code","colab":{},"outputId":"4ef894fc-057c-4c9e-b40f-73e40ea8e1f0"},"source":["plt.imshow(images[0][0].numpy(), 'gray')\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"0V7lOk_KiSIJ","colab_type":"code","colab":{},"outputId":"73f9ce80-2c21-4afe-a835-44ff1ed111af"},"source":["print(targets[0])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor(6)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rq9y9y2PiSIT","colab_type":"text"},"source":["## Building NN models\n","\n","We should inherit `torch.nn.Module` class to implement pytorch model, `torch.nn` has lots of pre-defined \"building bricks\".\n","\n","Their functional analogs are collected in `torch.nn.functional`."]},{"cell_type":"code","metadata":{"id":"3NHBExZSiSIV","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","import torch.nn.functional as F"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TLTQ3UJ3iSIo","colab_type":"text"},"source":["We have both module and functional versions of max-pooling, activations, upsempling and some other ops:\n","* `nn.MaxPool2d` / `F.max_pool2d`\n","* `nn.ReLU` / `F.relu`\n","* `nn.Upsample(mode='bilinar')` / `F.upsample(mode='bilinar')` — **deprecated**, use `F.interpolate` instead\n","\n","To compose several nn modules we can use `nn.Squential`:"]},{"cell_type":"code","metadata":{"id":"C8NYQOiwiSIr","colab_type":"code","colab":{},"outputId":"8d595537-e062-4e82-921b-f06478c2c1cb"},"source":["layers = [\n","    nn.Conv2d(3, 64, 3, padding=1),\n","    nn.ReLU(),\n","    nn.Conv2d(64, 64, 3, padding=1),\n","    nn.ReLU()\n","]\n","unet_down1 = nn.Sequential(*layers)\n","print(unet_down1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Sequential(\n","  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (1): ReLU()\n","  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (3): ReLU()\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hQYVIzefiSIv","colab_type":"text"},"source":["## Models training\n","\n","To train a model one must define a looss functions, some of them can also be found in `torch.nn`:\n","\n","```python\n","output = model(torch.cat(x, 1))\n","target = torch.arange(1, 1001)\n","criterion = nn.MSELoss()\n","\n","loss = criterion(output, target)\n","print(loss)\n","```"]},{"cell_type":"markdown","metadata":{"id":"_gGFJ7Y3iSI0","colab_type":"text"},"source":["We can update the weights manually:\n","\n","```python\n","learning_rate = 0.01\n","for f in net.parameters():\n","    f.data.sub_(f.grad.data * learning_rate)\n","```"]},{"cell_type":"markdown","metadata":{"id":"gQjBO6jEiSI1","colab_type":"text"},"source":["Or use built-in optimizer from `torch.optim` family:\n","```python\n","import torch.optim as optim\n","\n","optimizer = optim.SGD(net.parameters(), lr=0.01)\n","```"]},{"cell_type":"markdown","metadata":{"id":"ZgOchNrSiSI2","colab_type":"text"},"source":["We need to zero the gradients to disable gradient accumulations that happens by defult (`pytorch` sends its regards to `tf` rnn implementations):\n","```python\n","optimizer.zero_grad()\n","```"]},{"cell_type":"markdown","metadata":{"id":"rIQRqvkkiSI3","colab_type":"text"},"source":["After zeroing the gradients we can run both forward and backward stage with loss calculation in-between:\n","```python\n","output = net(x)\n","loss = criterion(output, target)\n","loss.backward()\n","```"]},{"cell_type":"markdown","metadata":{"id":"XAdLx8njiSI4","colab_type":"text"},"source":["To update the weights we can use the optimizer again:\n","```python\n","optimizer.step()\n","```"]},{"cell_type":"markdown","metadata":{"id":"07qyq41RiSI7","colab_type":"text"},"source":["To adjust learning rate in some pre-defined way we can use **`torch.optim.lr_scheduler`** sub-module:\n","```python\n","from torch.optim import lr_scheduler\n","```"]},{"cell_type":"markdown","metadata":{"id":"5kMbnhshiSI7","colab_type":"text"},"source":["To train ResNets on ImageNet with a standard training scheme we can use\n","```python\n","scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n","for epoch in range(100):\n","    # no scheduler.step() here* \n","    # train(...)\n","    # validate(...)\n","    scheduler.step()  # == scheduler.step(epoch)\n","```\n","\\* one **should not** put `scheduler.step()` before `optimizer.step()`, otherwise the first epoch will be \"skipped\"."]},{"cell_type":"markdown","metadata":{"id":"ey4VEPsjiSI8","colab_type":"text"},"source":["## Optional reading"]},{"cell_type":"markdown","metadata":{"id":"D1oGT7DRiSI9","colab_type":"text"},"source":["There are several libraries out there like [`catalyst`](https://github.com/catalyst-team/catalyst) or [`kekas`](https://github.com/belskikh/kekas) that make building pipelines even easier."]},{"cell_type":"markdown","metadata":{"id":"0y2oNF5xiSI9","colab_type":"text"},"source":["# Pytorch in the wils"]},{"cell_type":"markdown","metadata":{"id":"mXAlK3pziSI-","colab_type":"text"},"source":["## Segmentation model creation (with U-Net as an example)\n","\n","![U-Net scheme](images/unet.png)"]},{"cell_type":"code","metadata":{"id":"AgyTza6MiSI-","colab_type":"code","colab":{}},"source":["import torch\n","from torch import nn"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gK7CfnTXiSJO","colab_type":"text"},"source":["Pre-defined heavily-used convolutions:"]},{"cell_type":"code","metadata":{"id":"tjQWKLgGiSJP","colab_type":"code","colab":{}},"source":["def conv3x3(in_channels, out_channels, dilation=1):\n","    return nn.Conv2d(in_channels, out_channels, 3, padding=dilation, dilation=dilation)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aIhegcISiSJW","colab_type":"text"},"source":["One **encoder block** consists of two sequential convolutions, an activation layer and an optional batch-norm:"]},{"cell_type":"code","metadata":{"id":"8F6PZIDniSJW","colab_type":"code","colab":{}},"source":["class EncoderBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, batch_norm=False):\n","        super().__init__()\n","\n","        self.batch_norm = batch_norm\n","\n","        self.conv1 = conv3x3(in_channels, out_channels)\n","        if self.batch_norm:\n","            self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.relu1 = nn.ReLU()\n","        self.conv2 = conv3x3(out_channels, out_channels)\n","        if self.batch_norm:\n","            self.bn2 = nn.BatchNorm2d(out_channels)\n","        self.relu2 = nn.ReLU()\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        if self.batch_norm:\n","            x = self.bn1(x)\n","        x = self.relu1(x)\n","        x = self.conv2(x)\n","        if self.batch_norm:\n","            x = self.bn2(x)\n","        x = self.relu2(x)\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"prtOrvD4iSJY","colab_type":"code","colab":{},"outputId":"046dc8cb-e085-4266-c964-97156c51f830"},"source":["block = EncoderBlock(3, 64)\n","block"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["EncoderBlock(\n","  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (relu1): ReLU()\n","  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (relu2): ReLU()\n",")"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"code","metadata":{"id":"VY39L1DKiSJb","colab_type":"code","colab":{},"outputId":"f89a93c6-ec0b-4403-9070-af08ef2bc7e4"},"source":["x = torch.zeros(4, 3, 128, 128)\n","\n","with torch.no_grad():\n","    print(block(x).shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["torch.Size([4, 64, 128, 128])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dNU3XnOXiSJe","colab_type":"text"},"source":["Alternative definition:"]},{"cell_type":"code","metadata":{"id":"6IlAxQf0iSJf","colab_type":"code","colab":{}},"source":["class EncoderBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, batch_norm=False):\n","        super().__init__()\n","\n","        self.block = nn.Sequential()\n","        self.block.add_module('conv1', conv3x3(in_channels, out_channels))\n","        if batch_norm:\n","            self.block.add_module('bn1', nn.BatchNorm2d(out_channels))\n","        self.block.add_module('relu1', nn.ReLU())\n","        self.block.add_module('conv2', conv3x3(out_channels, out_channels))\n","        if batch_norm:\n","            self.block.add_module('bn2', nn.BatchNorm2d(out_channels))\n","        self.block.add_module('relu2', nn.ReLU())\n","\n","    def forward(self, x):\n","        return self.block(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pqymd6qhiSJn","colab_type":"code","colab":{},"outputId":"ab8a674e-f931-4ef2-d16b-1faaa9443c34"},"source":["block = EncoderBlock(3, 64)\n","block"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["EncoderBlock(\n","  (block): Sequential(\n","    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (relu1): ReLU()\n","    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (relu2): ReLU()\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"code","metadata":{"id":"ic7gpSf0iSJq","colab_type":"code","colab":{},"outputId":"5fb70f7c-ed83-458e-dacd-66993a2dab71"},"source":["x = torch.zeros(4, 3, 128, 128)\n","\n","with torch.no_grad():\n","    print(block(x).shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["torch.Size([4, 64, 128, 128])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"--mqQG22iSJs","colab_type":"text"},"source":["And its \"functional\" version:"]},{"cell_type":"code","metadata":{"id":"VnCQutgxiSJs","colab_type":"code","colab":{}},"source":["def encoder_block(in_channels, out_channels, batch_norm=False):\n","    block = nn.Sequential()\n","    block.add_module('conv1', conv3x3(in_channels, out_channels))\n","    if batch_norm:\n","        block.add_module('bn1', nn.BatchNorm2d(out_channels))\n","    block.add_module('relu1', nn.ReLU())\n","    block.add_module('conv2', conv3x3(out_channels, out_channels))\n","    if batch_norm:\n","        block.add_module('bn2', nn.BatchNorm2d(out_channels))\n","    block.add_module('relu2', nn.ReLU())\n","    return block"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g6JJXxZ0iSJu","colab_type":"code","colab":{},"outputId":"f70fe66f-1277-4bc1-dde9-2e3548bb35c5"},"source":["block = encoder_block(3, 64)\n","block"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Sequential(\n","  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (relu1): ReLU()\n","  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (relu2): ReLU()\n",")"]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"code","metadata":{"id":"iVWylrp3iSJ4","colab_type":"code","colab":{},"outputId":"d8337d40-806e-46ec-ae86-b6d6d414dfdc"},"source":["x = torch.zeros(4, 3, 128, 128)\n","\n","with torch.no_grad():\n","    print(block(x).shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["torch.Size([4, 64, 128, 128])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sVzFr3XNiSJ6","colab_type":"text"},"source":["Encoder is composed from several encoder blocks.\n","\n","Its final form is defined bu the number of input channels, the number of channels in the first block output and the number of such blocks.\n","\n","And we need to store preliminary activatons to apply Decoder on."]},{"cell_type":"code","metadata":{"id":"MlL4wX0EiSJ7","colab_type":"code","colab":{}},"source":["class Encoder(nn.Module):\n","    def __init__(self, in_channels, num_filters, num_blocks):\n","        super().__init__()\n","\n","        self.num_blocks = num_blocks\n","        for i in range(num_blocks):\n","            in_channels = in_channels if not i else num_filters * 2 ** (i - 1)\n","            out_channels = num_filters * 2**i\n","            self.add_module(f'block{i + 1}', encoder_block(in_channels, out_channels))\n","            if i != num_blocks - 1:\n","                self.add_module(f'pool{i + 1}', nn.MaxPool2d(2, 2))\n","\n","    def forward(self, x):\n","        acts = []\n","        for i in range(self.num_blocks):\n","            x = self.__getattr__(f'block{i + 1}')(x)\n","            acts.append(x)\n","            if i != self.num_blocks - 1:\n","                x = self.__getattr__(f'pool{i + 1}')(x)\n","        return acts"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dgwVTG0xiSKC","colab_type":"text"},"source":["Here we can use **`add_module`** way of layers definition as its number is variable."]},{"cell_type":"code","metadata":{"id":"7b-KwLrFiSKK","colab_type":"code","colab":{},"outputId":"15a57ae0-e30c-48dd-bb2d-912d48454e7a"},"source":["encoder = Encoder(in_channels=3, num_filters=8, num_blocks=4)\n","encoder"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Encoder(\n","  (block1): Sequential(\n","    (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (relu1): ReLU()\n","    (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (relu2): ReLU()\n","  )\n","  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (block2): Sequential(\n","    (conv1): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (relu1): ReLU()\n","    (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (relu2): ReLU()\n","  )\n","  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (block3): Sequential(\n","    (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (relu1): ReLU()\n","    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (relu2): ReLU()\n","  )\n","  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (block4): Sequential(\n","    (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (relu1): ReLU()\n","    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (relu2): ReLU()\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":62}]},{"cell_type":"code","metadata":{"id":"QQfZUesyiSKN","colab_type":"code","colab":{},"outputId":"9085256a-696f-4ada-d307-521c92d17990"},"source":["x = torch.zeros(4, 3, 512, 512)\n","\n","[_.shape for _ in encoder(x)]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[torch.Size([4, 8, 512, 512]),\n"," torch.Size([4, 16, 256, 256]),\n"," torch.Size([4, 32, 128, 128]),\n"," torch.Size([4, 64, 64, 64])]"]},"metadata":{"tags":[]},"execution_count":63}]},{"cell_type":"markdown","metadata":{"id":"lIMwI3VfiSKa","colab_type":"text"},"source":["Decoder block consists of upscaling \"lower\" output and concatenating it with a saved encoder block output from the \"left\"."]},{"cell_type":"code","metadata":{"id":"wZCPRJrmiSKb","colab_type":"code","colab":{}},"source":["Upsample = nn.Upsample\n","\n","# class Upsample(nn.Module):\n","#     def __init__(self, scale_factor=2, mode='bilinear'):\n","#         super().__init__()\n","\n","#         self.scale_factor = scale_factor\n","#         self.mode = mode\n","\n","#     def forward(self, x):\n","#         return F.interpolate(x, scale_factor=self.scale_factor, mode=self.mode, align_corners=True)\n","\n","class DecoderBlock(nn.Module):\n","    def __init__(self, out_channels):\n","        super().__init__()\n","\n","        self.uppool = Upsample(scale_factor=2, mode='bilinear')\n","        self.upconv = conv3x3(out_channels * 2, out_channels)\n","        self.conv1 = conv3x3(out_channels * 2, out_channels)\n","        self.conv2 = conv3x3(out_channels, out_channels)\n","\n","    def forward(self, down, left):\n","        x = self.uppool(down)\n","        x = self.upconv(x)\n","        x = torch.cat([left, x], 1)\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LwSLBpEAiSKc","colab_type":"code","colab":{}},"source":["block = DecoderBlock(8)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tJFOBpqViSKe","colab_type":"code","colab":{}},"source":["y = encoder(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1GVZKoHNiSKt","colab_type":"code","colab":{},"outputId":"6f5ca640-e516-4752-897d-ba32d9a06540"},"source":["y[1].shape, y[0].shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([4, 16, 256, 256]), torch.Size([4, 8, 512, 512]))"]},"metadata":{"tags":[]},"execution_count":67}]},{"cell_type":"code","metadata":{"id":"F5ccwKs5iSKx","colab_type":"code","colab":{},"outputId":"9ac01f04-0b34-4e8a-f231-07ff8922435a"},"source":["block(y[1], y[0]).shape"],"execution_count":0,"outputs":[{"output_type":"stream","text":["C:\\Apps\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2479: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode))\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 8, 512, 512])"]},"metadata":{"tags":[]},"execution_count":68}]},{"cell_type":"markdown","metadata":{"id":"Tc_-NVGxiSKz","colab_type":"text"},"source":["Let us build Decoder from several decoder blocks:"]},{"cell_type":"code","metadata":{"id":"rCGgJRnXiSK5","colab_type":"code","colab":{}},"source":["class Decoder(nn.Module):\n","    def __init__(self, num_filters, num_blocks):\n","        super().__init__()\n","\n","        for i in range(num_blocks):\n","            self.add_module(f'block{num_blocks - i}', DecoderBlock(num_filters * 2**i))\n","\n","    def forward(self, acts):\n","        up = acts[-1]\n","        for i, left in enumerate(acts[-2::-1]):\n","            up = self.__getattr__(f'block{i + 1}')(up, left)\n","        return up"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dj-IibxOiSK7","colab_type":"code","colab":{}},"source":["decoder = Decoder(8, 3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"619QZkOriSK9","colab_type":"code","colab":{},"outputId":"b7431694-5425-4ab3-c1e0-a1b21128501f"},"source":["x.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 3, 512, 512])"]},"metadata":{"tags":[]},"execution_count":71}]},{"cell_type":"code","metadata":{"id":"U10rnXhfiSK_","colab_type":"code","colab":{},"outputId":"2487ef4a-743c-4ef5-b84e-09b7fab7316c"},"source":["[_.shape for _ in encoder(x)]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[torch.Size([4, 8, 512, 512]),\n"," torch.Size([4, 16, 256, 256]),\n"," torch.Size([4, 32, 128, 128]),\n"," torch.Size([4, 64, 64, 64])]"]},"metadata":{"tags":[]},"execution_count":72}]},{"cell_type":"code","metadata":{"id":"NHmgwxKxiSLA","colab_type":"code","colab":{},"outputId":"52bba260-4132-4645-ffbe-0f6c5b7a1e69"},"source":["decoder(encoder(x)).shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 8, 512, 512])"]},"metadata":{"tags":[]},"execution_count":73}]},{"cell_type":"markdown","metadata":{"id":"mpCmBOBhiSLC","colab_type":"text"},"source":["U-Net is build from Encoder, Decoder and a final classification layer:"]},{"cell_type":"code","metadata":{"id":"tek56ElHiSLC","colab_type":"code","colab":{}},"source":["class UNet(nn.Module):\n","    def __init__(self, num_classes, in_channels=3, num_filters=64, num_blocks=4):\n","        super().__init__()\n","\n","        print(f'=> Building {num_blocks}-blocks {num_filters}-filter U-Net')\n","\n","        self.encoder = Encoder(in_channels, num_filters, num_blocks)\n","        self.decoder = Decoder(num_filters, num_blocks - 1)\n","        self.final = nn.Conv2d(num_filters, num_classes, 1)\n","\n","    def forward(self, x):\n","        acts = self.encoder(x)\n","        x = self.decoder(acts)\n","        x = self.final(x)\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2hpvRebviSLD","colab_type":"text"},"source":["\"Integration testing\" (pytorch 0.3 legacy code):"]},{"cell_type":"code","metadata":{"id":"Qr7_9C38iSLE","colab_type":"code","colab":{},"outputId":"2100cca3-5de6-401d-ea03-23031b64771e"},"source":["from torch.autograd import Variable\n","\n","model = UNet(num_classes=1)\n","#if torch.cuda.is_available():\n"," #   model.cuda()\n","\n","images = Variable(torch.randn(4, 3, 416, 416), volatile=True)\n","#if torch.cuda.is_available():\n","#    images = images.cuda()\n","\n","model.forward(images).shape"],"execution_count":0,"outputs":[{"output_type":"stream","text":["=> Building 4-blocks 64-filter U-Net\n"],"name":"stdout"},{"output_type":"stream","text":["C:\\Apps\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  import sys\n"],"name":"stderr"},{"output_type":"error","ename":"RuntimeError","evalue":"[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 1594884096 bytes. Buy new RAM!\n","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32m<ipython-input-76-cc97b2dfaaa8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#    images = images.cuda()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;32m<ipython-input-74-48a74dde4ee4>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0macts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\Apps\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m<ipython-input-69-dcb7ae32f49b>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, acts)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0macts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'block{i + 1}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\Apps\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m<ipython-input-64-7957558e94e0>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, down, left)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\Apps\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\Apps\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\Apps\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    338\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    339\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[1;32m--> 340\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 1594884096 bytes. Buy new RAM!\n"]}]},{"cell_type":"markdown","metadata":{"id":"yAJpv5iuiSLK","colab_type":"text"},"source":["```\n","/home/nizhib/.anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  import sys\n","/home/nizhib/.anaconda3/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n","  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n","/home/nizhib/.anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:2423: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode))\n","```"]},{"cell_type":"code","metadata":{"id":"vywVSRcliSLM","colab_type":"code","colab":{},"outputId":"2897a205-e77c-430e-e66d-58ba0639fa35"},"source":[""],"execution_count":0,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'Tkinter'","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32m<ipython-input-1-b1b1bdeeb10f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mTkinter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;31m#Подключаем модуль Tkinter в наше приложение\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Производим инициализацию нашего графического интерфейса\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcanvas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCanvas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Инициализируем Canvas размером 300х300 пикселей\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Размещаем Canvas в окне нашего Tkinter-GUI\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mroot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Создаем постоянный цикл\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Tkinter'"]}]},{"cell_type":"markdown","metadata":{"id":"nl26amMTiSLQ","colab_type":"text"},"source":["Same for **pytorch 0.4+**:"]},{"cell_type":"code","metadata":{"id":"tzl9hhDfiSLR","colab_type":"code","colab":{},"outputId":"d486d75c-d47d-406a-f23d-25d71449452d"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model = UNet(num_classes=1)\n","model.to(device)\n","\n","images = torch.randn(4, 3, 416, 416).to(device)\n","\n","with torch.no_grad():\n","    print(model.forward(images).shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["=> Building 4-blocks 64-filter U-Net\n"],"name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"CUDA out of memory. Tried to allocate 170.00 MiB (GPU 0; 4.00 GiB total capacity; 2.79 GiB already allocated; 122.77 MiB free; 40.76 MiB cached)","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32m<ipython-input-86-c8966fbe28a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;32m<ipython-input-84-48a74dde4ee4>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0macts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\Apps\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m<ipython-input-71-894b72c146fa>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0macts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'block{i + 1}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[0macts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_blocks\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\Apps\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\Apps\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\Apps\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\Apps\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\Apps\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    338\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    339\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[1;32m--> 340\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 170.00 MiB (GPU 0; 4.00 GiB total capacity; 2.79 GiB already allocated; 122.77 MiB free; 40.76 MiB cached)"]}]},{"cell_type":"markdown","metadata":{"id":"OMQ9cj6_iSLS","colab_type":"text"},"source":["We get linear activations as an output.\n","\n","To train such models we need to use **/.\\*WithLogits/** loss functions subset\n","\n","We can use `torch.sigmoid` or `torch.softmax` to get probabilities (**0.4.1+**, `torch.nn.functional.sigmoid/softmax` before)"]},{"cell_type":"markdown","metadata":{"id":"fX4xX9T4iSLT","colab_type":"text"},"source":["## Using pretrained encoder"]},{"cell_type":"markdown","metadata":{"id":"xI8XyLpmiSLT","colab_type":"text"},"source":["Encoder blocks structure seen before looks exactly like VGG architecture:\n","\n","![img](https://www.pyimagesearch.com/wp-content/uploads/2017/03/imagenet_vgg16.png)\n","\n","Let us have a look at VGG model from `torchvision` library:"]},{"cell_type":"code","metadata":{"id":"csa3MI3riSLU","colab_type":"code","colab":{}},"source":["from torchvision.models import vgg13"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fMWYYZr7iSLe","colab_type":"text"},"source":["VGG13 is the VGG version with 2 convolutional layer in each block:"]},{"cell_type":"code","metadata":{"id":"iJzONFJKiSLe","colab_type":"code","colab":{}},"source":["model = vgg13()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R33elGtBiSLi","colab_type":"code","colab":{},"outputId":"3fb97f4e-c426-49d1-c3e4-dd280a1eee5b"},"source":["model"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["VGG(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (6): ReLU(inplace=True)\n","    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): ReLU(inplace=True)\n","    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): ReLU(inplace=True)\n","    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (13): ReLU(inplace=True)\n","    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (16): ReLU(inplace=True)\n","    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (18): ReLU(inplace=True)\n","    (19): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (20): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (21): ReLU(inplace=True)\n","    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (23): ReLU(inplace=True)\n","    (24): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n","  (classifier): Sequential(\n","    (0): Linear(in_features=25088, out_features=4096, bias=True)\n","    (1): ReLU(inplace=True)\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): Linear(in_features=4096, out_features=4096, bias=True)\n","    (4): ReLU(inplace=True)\n","    (5): Dropout(p=0.5, inplace=False)\n","    (6): Linear(in_features=4096, out_features=1000, bias=True)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"markdown","metadata":{"id":"t6gOsQ-8iSLu","colab_type":"text"},"source":["We don't need the classifier, only the features are useful.\n","\n","They are built from conv-relu-conv-relu + maxpooling.\n","\n","Let's build encoder blocks via grouping VGG layers:"]},{"cell_type":"code","metadata":{"id":"q7pqQ8p2iSLv","colab_type":"code","colab":{}},"source":["class VGG13Encoder(nn.Module):\n","    def __init__(self, num_blocks, pretrained=True):\n","        super().__init__()\n","\n","        backbone = vgg13(pretrained=pretrained).features\n","\n","        self.num_blocks = num_blocks\n","        for i in range(self.num_blocks):\n","            block = nn.Sequential(*[backbone[j] for j in range(i * 5, i * 5 + 4)])\n","            self.add_module(f'block{i + 1}', block)\n","            if i != num_blocks - 1:\n","                self.add_module(f'pool{i + 1}', nn.MaxPool2d(2, 2))\n","\n","    def forward(self, x):\n","        acts = []\n","        for i in range(self.num_blocks):\n","            x = self.__getattr__(f'block{i + 1}')(x)\n","            acts.append(x)\n","            if i != self.num_blocks - 1:\n","                x = self.__getattr__(f'pool{i + 1}')(x)\n","        return acts"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nDtPyClhiSLw","colab_type":"code","colab":{},"outputId":"7f9455d0-cc1e-4f35-98df-1e9440ef4f7c"},"source":["vgg_encoder = VGG13Encoder(num_blocks=4, pretrained=False)\n","vgg_encoder"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["VGG13Encoder(\n","  (block1): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","  )\n","  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (block2): Sequential(\n","    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","  )\n","  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (block3): Sequential(\n","    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","  )\n","  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (block4): Sequential(\n","    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":58}]},{"cell_type":"markdown","metadata":{"id":"lu32e9n5iSMU","colab_type":"text"},"source":["Compare it to \"vanilla\" encoder:"]},{"cell_type":"code","metadata":{"id":"exvYeK4viSMY","colab_type":"code","colab":{},"outputId":"aba7ef8c-3d78-4669-fdf4-cf125bcf944a"},"source":["encoder = Encoder(in_channels=3, num_filters=64, num_blocks=4)\n","encoder"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Encoder(\n","  (block1): Sequential(\n","    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (relu1): ReLU()\n","    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (relu2): ReLU()\n","  )\n","  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (block2): Sequential(\n","    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (relu1): ReLU()\n","    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (relu2): ReLU()\n","  )\n","  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (block3): Sequential(\n","    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (relu1): ReLU()\n","    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (relu2): ReLU()\n","  )\n","  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (block4): Sequential(\n","    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (relu1): ReLU()\n","    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (relu2): ReLU()\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"markdown","metadata":{"id":"jRcblKu6iSMh","colab_type":"text"},"source":["Both structures are identical!\n","\n","But now we have some usefull pretrained weights in the decoder."]},{"cell_type":"markdown","metadata":{"id":"2a7g8amqiSMi","colab_type":"text"},"source":["## <font color='#cc6666'>Hometask!</font>\n","\n","### Part1. Toy dataset\n","\n","\n","**Implement** toy dataset to generate noisy ellipses like that:\n","\n","![img](https://raw.githubusercontent.com/jakeret/tf_unet/master/docs/toy_problem.png)"]},{"cell_type":"markdown","metadata":{"id":"fcIy4U2wiSMm","colab_type":"text"},"source":["The dataset should output both the image and its corresponding mask."]},{"cell_type":"code","metadata":{"id":"q_BYihIgiSMn","colab_type":"code","colab":{},"outputId":"b3111746-1f6c-46f0-a658-e956619af5a5"},"source":["class Ellipses(Dataset):\n","    pass"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'Dataset' is not defined","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m<ipython-input-60-c09fda913a86>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mEllipses\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mNameError\u001b[0m: name 'Dataset' is not defined"]}]},{"cell_type":"code","metadata":{"id":"vW3zGtpaiSMw","colab_type":"code","colab":{},"outputId":"3388f76b-ae6c-41f0-d0f7-6878d3ba9138"},"source":["\n"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"qqKTaiQHiSM4","colab_type":"code","colab":{},"outputId":"f1f0b011-46a9-45f3-cf89-d96fe0689cfe"},"source":["radius = np.random.random(NUM)*2\n","radius //= 1\n","radius += 1\n","radius"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 1.,  1.,  2.,  1.,  1.,  2.,  2.])"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"HKtxQRKdiSNA","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","def generator_P(P):\n","    NUM = np.random.randint(10)+1\n","    size = 512\n","    rad_size =  5 +np.random.randint(size//4)\n","    radius = np.random.random(NUM)*rad_size\n","    radius //= 1\n","    radius += 1\n","    bg = np.random.random(size=(1,3))\n","    #print(radius)\n","    clr = np.random.random(size=(NUM,3)) \n","    #print(clr)\n","    x = np.random.random(NUM)*(size - 2*rad_size)\n","    x //= 1\n","    x += 1\n","    y = np.random.random(NUM)*(size - 2*rad_size)\n","    y //= 1\n","    y += 1\n","    mask = np.zeros((size,size))\n","    img = np.ones((size,size,3))\n","    img -= bg \n","    for k in range(NUM):\n","        for i in range(2*int(radius[k])):\n","            for j in range (2*int(radius[k])):\n","                if (((radius[k] - i -1 )**2 + (radius[k] - j -1)**2) < (radius[k] -1)**2) :\n","                    mask[x[k] + i][y[k] + j] = 1\n","                    for h in range(3):\n","                        img[x[k] + i][y[k] + j][h] = clr[k][h]\n","\n","\n","\n","    #mask[3:-3, 4:-3] = 1 # white square in black background\n","    img +=  np.random.randn(size,size,3) * 0.1 # random image\n","    #masked = np.ma.masked_where(mask == 0, mask)\n","    \n","    \n","    #print(img)\n","    cmap = plt.cm.RdYlBu\n","    plt.figure(figsize = (15,15) , dpi = 70)\n","    plt.imshow(mask, 'gray', interpolation='none')\n","    #plt.show()\n","    plt.savefig('./data/mask/{}.png'.format(P))\n","    plt.figure(figsize = (15,15) , dpi = 70)\n","    plt.imshow(img, 'gray', interpolation='gaussian'  )\n","    #plt.imshow(masked, 'Blues', interpolation ='nearest' , alpha=1)\n","    plt.savefig('./data/img/{}.png'.format(P))\n","    plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"szP7gJKFiSND","colab_type":"text"},"source":["Define utility function:"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"MPXlU7bEiSNE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":299},"outputId":"b8c13de3-5670-4dd0-e9da-d94859701e98","executionInfo":{"status":"error","timestamp":1570978602953,"user_tz":-180,"elapsed":429,"user":{"displayName":"тимур аминов","photoUrl":"","userId":"10861342555234053496"}}},"source":["for i in range(1448, 5001):\n","    generator_P(i)"],"execution_count":11,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-781416a64805>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1448\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mgenerator_P\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-10-a8bdd6e506c4>\u001b[0m in \u001b[0;36mgenerator_P\u001b[0;34m(P)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mradius\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mradius\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mradius\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mradius\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                     \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                         \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"]}]},{"cell_type":"code","metadata":{"id":"vWlDiCGMiSNJ","colab_type":"code","colab":{}},"source":["import hashlib\n","\n","def moy_variant(surname):\n","    return int(hashlib.md5(surname.encode().lower()).hexdigest()[-1], 16) % 2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"C5dIbm2OiSNQ","colab_type":"code","colab":{},"outputId":"e34a76c6-8ec2-4e67-c71d-056a44683989"},"source":["moy_variant('аминов')"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{"tags":[]},"execution_count":87}]},{"cell_type":"markdown","metadata":{"id":"fk6jfUkiiSNS","colab_type":"text"},"source":["**Implement ACFNet** (https://arxiv.org/abs/1909.09408, if `moy_variant` returns 1 for you) **or OCRNet** (https://arxiv.org/abs/1909.11065, otherwise) based on pretrained **DenseNet** (pytorch) and **DPN** (cadene/pretrainedmodels) networks instead of ResNets from the papers.\n","\n","It is advised to use `BCEWithLogitsLoss` as a loss function."]},{"cell_type":"code","metadata":{"id":"PdyuNv5MiSNT","colab_type":"code","colab":{}},"source":["class ACFDenseNet(nn.Module):\n","    def __init__(self, num_classes, backbone='densenet161'):\n","        pass\n","\n","    def forward(self, x):\n","        pass\n","\n","\n","class ACFDPN(nn.Module):\n","    def __init__(self, num_classes, arch='dpn92'):\n","        pass\n","\n","    def forward(self, x):\n","        pass\n","\n","\n","class OCRDenseNet(nn.Module):\n","    def __init__(self, num_classes, arch='densenet161'):\n","        pass\n","\n","    def forward(self, x):\n","        pass\n","\n","\n","class OCRDPN(nn.Module):\n","    def __init__(self, num_classes, arch='dpn92'):\n","        pass\n","\n","    def forward(self, x):\n","        pass"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P_M6ip9DiSNV","colab_type":"text"},"source":["**Demonstrate** how they are trained with `tensorboard` screenshots (use `tensorboardX` library).\n","\n","That includes but not limited to loss curves, masks from different epochs etc."]},{"cell_type":"markdown","metadata":{"id":"kN4YwExxiSNZ","colab_type":"text"},"source":["### Part2. Portrait Segmentation\n","\n","Repeat the training procedures on some real task: http://xiaoyongshen.me/webpage_portrait/.\n","\n","Easy-to-use dataset version can be downloaded from https://yadi.sk/d/1SSkfLh4WnEhmw.\n","\n","Use `dice score` as a target scoring function, it should be published in the final report.\n","\n","All the loss charts, mask quality evolution and final mask examples are expected as well."]},{"cell_type":"markdown","metadata":{"id":"vtH01ruPiSNf","colab_type":"text"},"source":["### Part 3. Bells and whistles"]},{"cell_type":"markdown","metadata":{"id":"aAiUzwPxiSNg","colab_type":"text"},"source":["* Use at least 5 `albumentations` or `imgaug` augmentations on the data preparation step (up to 0.5 bonus points);\n","\n","\n","* Use `catalyst` or `kekas` to build the training pipeline (up to 0.5 bonus points);\n","\n","\n","* Deploy the web demo to play with on your own `dl2019fall-{lastname}.ml` domain.\n","\n","  The demo earns 0.1 bonus points for each day it's alive in the range from **Oct 14 00:00 to Oct 27 23:59**.\n","\n","  You can use https://github.com/nizhib/portrait-demo as a start point (up to 2 bonus points for 2-week streak)."]}]}